{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0b5806dc0e8395af2278a83b10faec683de35099ebc749f75874e48097d605efb",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import data_handler\n",
    "import machine_learning as ml\n",
    "import cost_benefit as cb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "## Main results for seminar paper\n",
    "This notebook is created to estimate the machine learning models for the chosen hyperparameters. See other notebook for invokement of the grid search algorithm to choose the optimal hyperparameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define wrapper to call all necessary functions.\n",
    "def result_wrapper(model, parameters, train, test, name):\n",
    "    # I have to create a fake class in order to make validate model work.\n",
    "    # This is because it was created with the purpose to work with the\n",
    "    # grid search algorithm.\n",
    "    x, y = train[0], train[1]\n",
    "\n",
    "    class Object(object):\n",
    "        pass\n",
    "    model_obj = Object()\n",
    "    model_obj.best_estimator_ = model(**parameters).fit(x, y)\n",
    "    best_model = ml.validate_model(model_obj, train, test, supress_print=True)\n",
    "    benefits = cb.cost_benefit_analysis(best_model, test)\n",
    "    print(f\"{name} & {benefits[0]:.2f} & {benefits[1]/benefits[0]*100:.2f} & {benefits[2]/benefits[0]*100:.2f} \\\\\\\\\")\n",
    "    print()\n",
    "    cb.latex_printout(best_model, test)"
   ]
  },
  {
   "source": [
    "### Ohlson's Logit\n",
    "I first define a function to run Ohlson's logit function, with the specified variables. I then evaluate it both in it's success at predicting bankruptcies, but also how much savings we expect."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohlson_data(year):\n",
    "    x_train, x_test, y_train, y_test = data_handler.load_data(year)\n",
    "    n = y_train.size\n",
    "    x_train.columns = data_handler.ohlson_varnames()\n",
    "    x_test.columns = data_handler.ohlson_varnames()\n",
    "\n",
    "    # Ohlson use a dummy if liabilites are greater than assets\n",
    "    x_train['liabilites > assets'] = (x_train['total liabilities / total assets'] > 1)*1.0\n",
    "    x_test['liabilites > assets'] = (x_test['total liabilities / total assets'] > 1)*1.0\n",
    "\n",
    "    # Try to replicate the variables as closely as possible,\n",
    "    # some of the variables are inverted, but that should not affect\n",
    "    # the predictability, only the sign of the coefficient.\n",
    "    ohlson_vars = [\n",
    "        'logarithm of total assets', \n",
    "        'total liabilities / total assets', \n",
    "        'working capital / total assets', \n",
    "        'current assets / short-term liabilities',\n",
    "        'liabilites > assets',\n",
    "        'net profit / total assets',\n",
    "        'total liabilities / ((profit on operating activities + depreciation) * (12/365))',\n",
    "        'sales (n) / sales (n-1)'\n",
    "    ]\n",
    "    x_ohlson_train = x_train[ohlson_vars]\n",
    "    x_ohlson_test = x_test[ohlson_vars]\n",
    "    return x_ohlson_train, x_ohlson_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ohlson's logit & 68.71 & 0.10 & 0.17 \\\\\n\n\\begin{tabular}{lrr}\n\\hline\n              &   Non-Bankrupt &   Bankrupt \\\\\n\\hline\n Non-Bankrupt &           1095 &          5 \\\\\n Bankrupt     &             79 &          3 \\\\\n\\hline\n\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = ohlson_data(5)\n",
    "ohlson_parameters = {'penalty': 'none', 'max_iter': 1000}\n",
    "result_wrapper(LogisticRegression, ohlson_parameters, (x_train, y_train), (x_test, y_test), \"Ohlson's logit\")"
   ]
  },
  {
   "source": [
    "### Logit with elastic net\n",
    "This is an extension of Ohlson's logit model. This time we include all available variables. Since this can lead to overfitting, we use elastic-net to reduce this problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ohlson's logit & 68.71 & 0.46 & 0.77 \\\\\n\n\\begin{tabular}{lrr}\n\\hline\n              &   Non-Bankrupt &   Bankrupt \\\\\n\\hline\n Non-Bankrupt &           1092 &          8 \\\\\n Bankrupt     &             73 &          9 \\\\\n\\hline\n\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = data_handler.load_data(5, out_frame=False)\n",
    "logit_parameters = {\n",
    "    'C': 1.2, \n",
    "    'l1_ratio': 0,\n",
    "    'penalty': 'elasticnet', \n",
    "    'solver': 'saga',\n",
    "    'max_iter': 10000\n",
    "}\n",
    "result_wrapper(LogisticRegression, logit_parameters, (x_train, y_train), (x_test, y_test), \"Logit\")"
   ]
  },
  {
   "source": [
    "### Gradient Descent classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-9fd9446bf735>, line 6)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-9fd9446bf735>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    'min_weight_fraction_leaf': 0.0,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "GBC_parameters = {\n",
    "    \"learning_rate\": 0.9,\n",
    "    \"min_samples_split\": 0.005,\n",
    "    \"min_samples_leaf\": 0.005,\n",
    "    \"max_depth\": 8,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'min_impurity_decrease': 1e-07\n",
    "}\n",
    "result_wrapper(GradientBoostingClassifier, GBC_parameters, (x_train, y_train), (x_test, y_test), \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}